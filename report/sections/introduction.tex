\section{Introduction}
Instance-based learning algorithms remain fundamental tools in machine learning, particularly for classification tasks where local patterns in the data are crucial for decision-making. These algorithms, while conceptually simple, can achieve remarkable performance when properly configured. However, their effectiveness often depends heavily on the choice of hyperparameters and the characteristics of the input data.
This study addresses two critical challenges in instance-based learning: optimal algorithm configuration and efficient data storage. We focus on two widely-used algorithms, K-Nearest Neighbors (KNN) and Support Vector Machines (SVM), applying them to binary classification problems with different characteristics - a balanced dataset for mushroom classification and an imbalanced medical dataset for hepatitis prognosis. Additionally, we investigate three data reduction techniques (GCNN, EENTH, and DROP3) to address the storage and computational efficiency challenges inherent in instance-based learning.
Our research contributes to the field by providing:

\begin{enumerate}
	\item A systematic comparison of different distance metrics, weighting methods, and voting policies for KNN.
	\item An evaluation of various kernel functions and regularization parameters for SVM
	\item An analysis of the effectiveness and trade-offs of different data reduction techniques
	\item Statistical validation of the results using rigorous hypothesis testing
\end{enumerate}