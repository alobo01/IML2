This study investigates the performance of instance-based learning algorithms, specifically K-Nearest Neighbors (KNN) and Support Vector Machines (SVM), on binary classification tasks using two distinct datasets: a balanced mushroom classification problem and an imbalanced medical dataset for hepatitis prognosis. We evaluate various configurations of these algorithms along with three data reduction techniques: Generalized Condensed Nearest Neighbor (GCNN), Elimination Editing with Nearest-neighbor Threshold (EENTH), and Decremental Reduction Optimization Procedure 3 (DROP3). Through comprehensive statistical analysis using Friedman tests and post-hoc analyses, we demonstrate that while KNN achieves perfect accuracy on the mushroom dataset regardless of configuration, the hepatitis dataset reveals significant differences between weighting methods. Our results indicate that the EENTH reduction method maintains classification performance while significantly reducing storage requirements, whereas GCNN shows performance degradation across both datasets. This research provides insights into the optimal configuration of instance-based learning algorithms and the effectiveness of different reduction techniques for real-world classification problems.